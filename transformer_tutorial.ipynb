{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a short notebook to walk through some of the applications of Transformer neural nets for tokenized analytic data, and demonstrate the functionality of the repo through examples. First let's load in the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import encoder_decoder, encoder_only, decoder_only\n",
    "from encoder_decoder import importLibs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each architecture does something a little different, roughly, the inputs and outputs look like the following:\n",
    "\n",
    "1. Encoder-Only: [3,4,2,5,...,5,6,3,4,3,3] --> 8\n",
    "2. Decoder-Only: [5,6,3,4,3] --> [5,6,3,4,3,3]\n",
    "3. Encoder-Decoder: [5,6,3,4,3,3] --> [3,4,2,5,1,3,2]\n",
    "\n",
    "In words, this looks like:\n",
    "\n",
    "1. Encoders take a sequence and maps it to a new vector in the embedding space that gets mapped to a single category\n",
    "2. Decoders take a sequence and predict the next token, either unconditionally, or...\n",
    "3. Encoder-Decoder conditions the next token predictino of the decoder layers with an encoder output vector. \n",
    "\n",
    "Obviously, these are all overlappping, and in many ways you can create the same behavior or encoders with decoders and vice-versa (just have the output of encoder map to the next token in the sequence, as opposed to some completely different semantic category). \n",
    "\n",
    "But for historical reasons, we'll keep all three of these architectures distinct as they have been used for different types of token prediction tasks.\n",
    "\n",
    "Let's start with encoder only and \"train\" a neural network to identify the largest token in a sequence -- i.e. effectively implement a MAX function acting on list using a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SequenceData(src,tgt)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
