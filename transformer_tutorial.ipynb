{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a short notebook to walk through some of the applications of Transformer neural nets for tokenized analytic data, and demonstrate the functionality of the repo through examples. First let's load in the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each architecture does something a little different, roughly, the inputs and outputs look like the following:\n",
    "\n",
    "1. Encoder-Only: [3,4,2,5,...,5,6,3,4,3,3] --> 8\n",
    "2. Decoder-Only: [[5,6,3,4,3,3],[8,2,3,5,1,3],...]\n",
    "3. Encoder-Decoder: [5,6,3,4,3,3] --> [3,4,2,5,1,3,2]\n",
    "\n",
    "In words, this looks like:\n",
    "\n",
    "1. Encoders take a sequence and maps it to a new vector in the embedding space that gets mapped to a single category\n",
    "2. Decoders take a sequence and predict the next token, it is thus trained on a set of sequences\n",
    "3. Encoder-Decoder conditions the next token predictino of the decoder layers with an encoder output vector. \n",
    "\n",
    "Obviously, these are all overlappping, and in many ways you can create the same behavior for encoders with decoders and vice-versa (just have the output of encoder map to the next token in the sequence, as opposed to some completely different semantic category). \n",
    "\n",
    "But for historical reasons, we'll keep all three of these architectures distinct as they have been used for different types of token prediction tasks.\n",
    "\n",
    "Let's start with encoder only and \"train\" a neural network to identify the largest token in a sequence -- i.e. effectively implement a MAX function acting on list using a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just run through each step of what makes a transformer. This will help us better understand what functions we need to write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24, 76, 55, 44, 94, 72])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "src = torch.randint(100,[6])\n",
    "encoder_layer = torch.nn.TransformerEncoderLayer(d_model = 16, nhead = 4, dim_feedforward = 16, dropout = 0)\n",
    "encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers = 4)\n",
    "encoder_embedding = torch.nn.Embedding(100,16)\n",
    "print(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will let the encoder have dropout = 0 so that the model produces a consistent output for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2127,  1.6537,  0.0723,  1.3046,  0.2210,  1.5384,  0.5104, -1.4163,\n",
      "         -0.4024, -1.4259,  0.4463, -0.9402, -1.1685, -0.3235,  0.5261,  0.6167],\n",
      "        [-0.5606,  0.8476, -0.6768,  0.5698,  0.2271,  1.2122, -0.2183, -1.6810,\n",
      "         -0.2648, -0.4991,  0.9850, -2.1261, -0.2526,  1.8321, -0.1312,  0.7368],\n",
      "        [ 1.1186,  1.7833,  0.9453, -1.3795,  0.8694, -0.4449, -1.0556,  0.4666,\n",
      "         -2.0631, -1.0185, -0.0567,  0.5114,  0.2645,  0.0031, -0.6264,  0.6822],\n",
      "        [ 0.9288,  0.9889,  0.2136,  0.9784, -1.1983,  0.7845,  1.0701, -1.5109,\n",
      "         -1.6719,  0.1763, -0.3626, -0.4841, -0.8155,  0.2127, -0.9847,  1.6748],\n",
      "        [ 1.6971,  1.1108,  0.8574, -0.7106, -0.0139,  0.3790, -1.2149, -1.1007,\n",
      "         -1.9908, -0.7293, -0.0161, -0.5163, -0.0650,  1.0827, -0.1328,  1.3633],\n",
      "        [ 1.8538,  1.2696,  0.6135,  0.5131, -1.2424,  0.8327, -0.2386,  0.2915,\n",
      "         -1.6153, -0.3127, -0.2418,  0.1944, -1.6435,  0.6072, -1.3548,  0.4733]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = encoder_embedding(src)\n",
    "internal_rep = encoder(x)\n",
    "print(internal_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what we have done here. We have taken a list of integers (tokens), mapped it to the embedding space (now a list of vectors), and transformed those vectors using our encoder layers (which are a combination of self-attention and feed-forward networks). Below are the shapes of the data at each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src size is: torch.Size([6])\n",
      "embedded input is:  torch.Size([6, 16])\n",
      "encoded input is:  torch.Size([6, 16])\n"
     ]
    }
   ],
   "source": [
    "print(\"src size is:\", src.shape)\n",
    "print(\"embedded input is: \",encoder_embedding(src).shape)\n",
    "print(\"encoded input is: \",internal_rep.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we just had an encoder only transformer, we are essentially done. We can take these encoded vectors, and map them to a new token space. Let's try this out by first performing the contraction on the sequence dimension (dim=0), and then on the embedding space (dim=1). We will do this with two-linear projection layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_projection = torch.nn.Linear(6,1)\n",
    "embedding_projection = torch.nn.Linear(16, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this we can now build probabilities that classify our internal representation into two outcomes, to which we will assign probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47317489981651306], [0.5268250703811646]]\n"
     ]
    }
   ],
   "source": [
    "x = embedding_projection(internal_rep)\n",
    "output = token_projection(x.transpose(0,1))\n",
    "softmax = torch.nn.Softmax(dim=0)\n",
    "\n",
    "print(softmax(output).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're cooking. Likewise, we could also pass this through a decoder stack, that maps the sequence to a desired output sequence. Let's give this a try by first creating instances of decoder_embedding, and decoder_layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = torch.nn.TransformerDecoderLayer(d_model = 16, nhead = 4, dim_feedforward = 16, dropout = 0)\n",
    "decoder = torch.nn.TransformerDecoder(decoder_layer, num_layers = 4)\n",
    "decoder_embedding = torch.nn.Embedding(100,16)\n",
    "final_projection = torch.nn.Linear(16, 9)\n",
    "tgt = torch.randint(100,[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 3, 3, 8, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "tgt_emb = decoder_embedding(tgt)\n",
    "decoded_seq = decoder(tgt_emb, internal_rep)\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "proj = final_projection(decoded_seq)\n",
    "print(softmax(proj).argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We've just gone through the entire mapping of a transformer. Now what we want to build is something that can do translation --> take some high complexity integral, and translate it to a basis of master integral weighted by integer coefficients over a large prime field. Simple enough. In  practice it will look something like this:\n",
    "\n",
    "model = EncoderDecoderModel(arg1,arg2,...)\n",
    "\n",
    "\"{3;5;2;3}\" --> \"{2341;6734;98432;325}\"\n",
    "\n",
    "We want this model to learn how to take the input sequence,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{', '3', ';', '5', ';', '2', ';', '3', '}']\n"
     ]
    }
   ],
   "source": [
    "print(list(\"{3;5;2;3}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and translate it into a sequence of the form,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{', '2', '3', '4', '1', ';', '6', '7', '3', '4', ';', '9', '8', '4', '3', '2', ';', '3', '2', '5', '}']\n"
     ]
    }
   ],
   "source": [
    "print(list(\"{2341;6734;98432;325}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve this, we will make the base model an autoregressive encoder-decoder transformer, while the high level model that we send sequences to will give the output once the loop is terminated. Here's a rough mock-up:\n",
    "\n",
    "base_model = EncoderDecoderModel(arg1,arg2,...)\n",
    "\n",
    "where base_model is autoregressive and takes in a src (input of the full model), and a tgt_t with some non-trivial entries. We will train the base_model, and then the full model simply runs a routine to generate the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AI_IBP_model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, base_model):\n",
    "        super().__init___()\n",
    "        self.initial_tgt = torch.tensor([\"{\",\"PAD\",\"PAD\",\"PAD\",\"PAD\",\"PAD\",...])\n",
    "        self.output_max_length = len(self.initial_tgt)\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def forward(self, src):\n",
    "        tgt = self.initial_tgt\n",
    "        src = torch.tensor(src)\n",
    "        for i in range(self.output_max_length-1):\n",
    "            tgt[i+1] = self.base_model(src, tgt)[i+1].tolist()\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be clear, the base_model is what gets trained with back propagation, and the high-level model AI_IBP_model is what is actually used to map one integral sequence to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's think about the training run. We will train the base model, that does autoregressive decoding. That means, for every pair of input_seq and output_seq, we want to prepare a data set with len(output_seq) training examples. For each example, we want the model to predict the token output_seq[i], given the input data input_seq, output_seq[i-1]. \n",
    "\n",
    "Let's do this one step at a time to get a sense for what we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder_decoder import EncoderDecoderModel\n",
    "\n",
    "base = EncoderDecoderModel(10,20,64,4,2,64)\n",
    "\n",
    "src = [1,6,3,4,5,2]\n",
    "tgt = [1,3,7,13,3,15,12,13,2]\n",
    "pad_tok = 0\n",
    "bos_tok = 1\n",
    "eos_tok = 2\n",
    "seq_len = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have made an instance of the EncoderDecoder model, and we're ready to cook. To standardize the training, we are going to normalize the input and output sequences, so that they all have seq_len = 10. \n",
    "\n",
    "To do this, we need to add padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  [1, 6, 3, 4, 5, 2, 0, 0, 0, 0]\n",
      "output:  [1, 3, 7, 13, 3, 15, 12, 13, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "def pad_seq(seq, max_len):\n",
    "    return seq +[pad_tok]*( max_len - len(seq))\n",
    "\n",
    "print(\"input: \",pad_seq(src, seq_len))\n",
    "print(\"output: \",pad_seq(tgt, seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1,  3,  7,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1,  3,  7, 13,  0,  0,  0,  0,  0],\n",
       "        [ 1,  3,  7, 13,  3,  0,  0,  0,  0],\n",
       "        [ 1,  3,  7, 13,  3, 15,  0,  0,  0],\n",
       "        [ 1,  3,  7, 13,  3, 15, 12,  0,  0],\n",
       "        [ 1,  3,  7, 13,  3, 15, 12, 13,  0],\n",
       "        [ 1,  3,  7, 13,  3, 15, 12, 13,  2]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_data = [tgt]\n",
    "for i in range(len(tgt)-1):\n",
    "    masked_data.append(tgt[:-i-1]+[0]*(i+1))\n",
    "masked_data.reverse()\n",
    "torch.tensor(masked_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than doing all of this in series (which is what our for-loop is doing), PyTorch as native masking functions that are well suited for parallel computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1,  3,  7,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1,  3,  7, 13,  0,  0,  0,  0,  0],\n",
       "        [ 1,  3,  7, 13,  3,  0,  0,  0,  0],\n",
       "        [ 1,  3,  7, 13,  3, 15,  0,  0,  0],\n",
       "        [ 1,  3,  7, 13,  3, 15, 12,  0,  0],\n",
       "        [ 1,  3,  7, 13,  3, 15, 12, 13,  0],\n",
       "        [ 1,  3,  7, 13,  3, 15, 12, 13,  2]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def causal_mask(tensor_input):\n",
    "    seq_length = tensor_input.size(1)  # Assume tensor_input has shape (batch_size, seq_length)\n",
    "    return torch.triu(torch.ones(seq_length,seq_length), diagonal=1) == 0\n",
    "torch.tensor(tgt).unsqueeze(0)*causal_mask(torch.tensor(tgt).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class EncoderDecoderModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, num_heads, num_layers, ff_dim, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder embedding and layer\n",
    "        self.encoder_embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, num_heads, ff_dim, dropout)\n",
    "\n",
    "        # Decoder embedding and layer\n",
    "        self.decoder_embedding = nn.Embedding(output_dim, d_model)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model, num_heads, ff_dim, dropout)\n",
    "\n",
    "        # Encoder/Decoder stack\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Source encoded with padding mask\n",
    "        src_emb = self.encoder_embedding(src)\n",
    "        src_padding_mask = padding_mask(src).to(src.device)\n",
    "        src_encoded = self.encoder(src_emb)\n",
    "\n",
    "        # Target decoded with causal & padding mask\n",
    "        tgt_emb = self.decoder_embedding(tgt)\n",
    "        tgt_padding_mask = padding_mask(tgt).to(src.device)\n",
    "        tgt_causal_mask = causal_mask(tgt).to(src.device)\n",
    "        tgt_decoded = self.decoder(tgt_emb, src_encoded)\n",
    "\n",
    "        return self.fc_out(tgt_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel = EncoderDecoderModel(10,20,64,4,2,64,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
